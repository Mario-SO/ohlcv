name: Performance Monitoring

on:
  schedule:
    # Run every day at 3:00 UTC for trend analysis
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      days_history:
        description: 'Number of days of history to analyze'
        required: false
        default: '30'
        type: string

jobs:
  performance-monitoring:
    name: Performance Trend Analysis
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for trend analysis
      
      - name: Setup Zig
        uses: goto-bus-stop/setup-zig@v2
        with:
          version: 0.14.1
      
      - name: Setup Python for analysis
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install Python dependencies
        run: |
          pip install matplotlib pandas numpy
      
      - name: Run current benchmark
        run: |
          mkdir -p monitoring_results
          echo "Running current benchmark..."
          zig build benchmark --summary none
          ./scripts/profile.sh benchmark
          
          # Save with timestamp
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          cp profiling_results/benchmark_*.txt monitoring_results/benchmark_$TIMESTAMP.txt
      
      - name: Download historical data
        uses: actions/download-artifact@v4
        with:
          name: performance-history
          path: monitoring_results/history/
        continue-on-error: true
      
      - name: Generate performance trend analysis
        run: |
          cat > analyze_trends.py << 'EOF'
          #!/usr/bin/env python3
          import os
          import re
          import json
          import pandas as pd
          import matplotlib.pyplot as plt
          from datetime import datetime, timedelta
          from pathlib import Path
          
          def parse_benchmark_file(file_path):
              """Parse a benchmark file and extract performance data"""
              try:
                  with open(file_path, 'r') as f:
                      content = f.read()
                  
                  # Extract timestamp from filename or content
                  filename = Path(file_path).name
                  if 'benchmark_' in filename:
                      timestamp_str = filename.replace('benchmark_', '').replace('.txt', '')
                      try:
                          timestamp = datetime.strptime(timestamp_str, '%Y%m%d_%H%M%S')
                      except:
                          timestamp = datetime.now()
                  else:
                      timestamp = datetime.now()
                  
                  # Extract performance metrics
                  pattern = r'(SMA-\d+|EMA-\d+|RSI-\d+|Bollinger-\d+|MACD|ATR-\d+):\s+([0-9.]+)\s+ms'
                  matches = re.findall(pattern, content)
                  
                  results = {'timestamp': timestamp}
                  for indicator, time_str in matches:
                      results[indicator] = float(time_str)
                  
                  return results
              except Exception as e:
                  print(f"Error parsing {file_path}: {e}")
                  return None
          
          def analyze_trends():
              """Analyze performance trends over time"""
              data_points = []
              
              # Parse all benchmark files
              for root, dirs, files in os.walk('.'):
                  for file in files:
                      if file.startswith('benchmark_') and file.endswith('.txt'):
                          file_path = os.path.join(root, file)
                          result = parse_benchmark_file(file_path)
                          if result:
                              data_points.append(result)
              
              if len(data_points) < 2:
                  print("âŒ Not enough historical data for trend analysis")
                  return
              
              # Create DataFrame
              df = pd.DataFrame(data_points)
              df = df.sort_values('timestamp').reset_index(drop=True)
              
              print("# ğŸ“ˆ Performance Trend Analysis")
              print(f"**Analysis Period:** {df['timestamp'].min()} to {df['timestamp'].max()}")
              print(f"**Data Points:** {len(df)}")
              print()
              
              # Analyze each indicator
              indicators = [col for col in df.columns if col != 'timestamp']
              
              print("## Performance Trends")
              print("| Indicator | Current (ms) | Previous (ms) | Trend | Change |")
              print("|-----------|--------------|---------------|-------|--------|")
              
              trend_summary = []
              
              for indicator in indicators:
                  if indicator in df.columns and not df[indicator].isna().all():
                      current = df[indicator].iloc[-1]
                      previous = df[indicator].iloc[0]
                      
                      change = current - previous
                      change_pct = (change / previous) * 100 if previous > 0 else 0
                      
                      if abs(change_pct) < 2:
                          trend = "ğŸ“Š Stable"
                      elif change_pct > 0:
                          trend = "ğŸ“ˆ Slower" if change_pct > 10 else "â¬†ï¸ Slightly Slower"
                          if change_pct > 25:
                              trend_summary.append(f"âš ï¸ {indicator}: {change_pct:+.1f}% performance degradation")
                      else:
                          trend = "ğŸ“‰ Faster" if change_pct < -10 else "â¬‡ï¸ Slightly Faster"
                          trend_summary.append(f"ğŸš€ {indicator}: {abs(change_pct):.1f}% performance improvement")
                      
                      print(f"| {indicator:<15} | {current:>8.3f} | {previous:>8.3f} | {trend} | {change_pct:>+6.1f}% |")
              
              print()
              
              if trend_summary:
                  print("## Notable Trends")
                  for trend in trend_summary:
                      print(f"- {trend}")
                  print()
              
              # Generate performance charts
              if len(df) > 3:
                  print("## Performance Charts")
                  
                  fig, axes = plt.subplots(2, 2, figsize=(15, 10))
                  fig.suptitle('OHLCV Performance Trends Over Time', fontsize=16)
                  
                  # Plot key indicators
                  key_indicators = ['SMA-20', 'EMA-20', 'RSI-14', 'MACD']
                  
                  for i, indicator in enumerate(key_indicators):
                      if indicator in df.columns:
                          ax = axes[i//2, i%2]
                          ax.plot(df['timestamp'], df[indicator], marker='o', linewidth=2)
                          ax.set_title(f'{indicator} Performance')
                          ax.set_ylabel('Time (ms)')
                          ax.grid(True, alpha=0.3)
                          ax.tick_params(axis='x', rotation=45)
                  
                  plt.tight_layout()
                  plt.savefig('performance_trends.png', dpi=150, bbox_inches='tight')
                  print("ğŸ“Š Performance chart saved as 'performance_trends.png'")
              
              # Save historical data
              df.to_json('performance_history.json', orient='records', date_format='iso')
              print("ğŸ’¾ Historical data saved to 'performance_history.json'")
          
          if __name__ == "__main__":
              analyze_trends()
          EOF
          
          python3 analyze_trends.py > performance_trends.md
          cat performance_trends.md
      
      - name: Update performance history
        run: |
          # Combine current results with historical data
          mkdir -p monitoring_results/updated_history
          
          # Copy existing history
          if [ -d "monitoring_results/history" ]; then
            cp monitoring_results/history/* monitoring_results/updated_history/ 2>/dev/null || true
          fi
          
          # Add current benchmark
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          cp profiling_results/benchmark_*.txt monitoring_results/updated_history/benchmark_$TIMESTAMP.txt
          
          # Keep only last 60 days of data
          find monitoring_results/updated_history -name "benchmark_*.txt" -mtime +60 -delete || true
      
      - name: Upload performance trends
        uses: actions/upload-artifact@v4
        with:
          name: performance-trends-${{ github.sha }}
          path: |
            performance_trends.md
            performance_trends.png
            performance_history.json
          retention-days: 90
      
      - name: Upload updated performance history
        uses: actions/upload-artifact@v4
        with:
          name: performance-history
          path: monitoring_results/updated_history/
          retention-days: 90
      
      - name: Create GitHub Issue for Performance Degradation
        if: contains(steps.*.outputs.*, 'performance degradation')
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let trendsReport = '';
            try {
              trendsReport = fs.readFileSync('performance_trends.md', 'utf8');
            } catch (error) {
              trendsReport = 'Failed to load performance trends report';
            }
            
            if (trendsReport.includes('performance degradation')) {
              const title = 'âš ï¸ Performance Degradation Detected';
              const body = `# Performance Degradation Alert
              
              Our automated performance monitoring has detected significant performance degradation in the OHLCV library.
              
              ${trendsReport}
              
              ## Recommended Actions
              
              1. Review recent commits that might have impacted performance
              2. Run local profiling to identify bottlenecks
              3. Consider performance optimizations
              4. Update benchmarks if intentional changes were made
              
              ## Monitoring Data
              
              - **Date:** ${new Date().toISOString()}
              - **Commit:** ${{ github.sha }}
              - **Workflow:** Performance Monitoring
              
              ---
              *This issue was automatically created by GitHub Actions performance monitoring*`;
              
              // Check if similar issue already exists
              const { data: issues } = await github.rest.issues.listForRepo({
                owner: context.repo.owner,
                repo: context.repo.repo,
                state: 'open',
                labels: 'performance'
              });
              
              const existingIssue = issues.find(issue => 
                issue.title.includes('Performance Degradation Detected')
              );
              
              if (!existingIssue) {
                await github.rest.issues.create({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  title: title,
                  body: body,
                  labels: ['performance', 'monitoring', 'bug']
                });
              }
            }

# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•